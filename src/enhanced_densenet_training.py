"""
å¢å¼ºç‰ˆ DenseNet-121 CheXpert æ¨¡å‹
åŒ…å«è®­ç»ƒåŠŸèƒ½å’ŒGrad-CAMå¯è§£é‡Šæ€§åˆ†æ
Supporting Slide 5 & 6: Deep Learning Training + Explainable AI
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import cv2
from pathlib import Path
import json

class FocalLoss(nn.Module):
    """
    ç„¦ç‚¹æŸå¤± - ä¸“é—¨å¤„ç†æåº¦ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
    Focal Loss for addressing class imbalance in multi-label classification
    """
    def __init__(self, alpha=1.0, gamma=2.0, pos_weight=None, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.pos_weight = pos_weight
        self.reduction = reduction
        
    def forward(self, inputs, targets):
        # è®¡ç®—æ ‡å‡†çš„BCEæŸå¤±
        bce_loss = nn.functional.binary_cross_entropy_with_logits(
            inputs, targets, pos_weight=self.pos_weight, reduction='none'
        )
        
        # è®¡ç®—æ¦‚ç‡
        probs = torch.sigmoid(inputs)
        
        # è®¡ç®—pt (æ­£ç¡®é¢„æµ‹çš„æ¦‚ç‡)
        pt = torch.where(targets == 1, probs, 1 - probs)
        
        # è®¡ç®—alphaæƒé‡
        alpha_t = torch.where(targets == 1, self.alpha, 1 - self.alpha)
        
        # åº”ç”¨ç„¦ç‚¹æƒé‡ (1-pt)^gamma
        focal_weight = alpha_t * (1 - pt) ** self.gamma
        
        # è®¡ç®—ç„¦ç‚¹æŸå¤±
        focal_loss = focal_weight * bce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

def calculate_class_weights(csv_file, labels):
    """
    è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„åŠ¨æ€æƒé‡ - ä¿®å¤ç‰ˆæœ¬
    ä½¿ç”¨æ›´ä¿å®ˆçš„æƒé‡è®¡ç®—ï¼Œé¿å…æç«¯å€¼
    """
    print("ğŸ” è®¡ç®—æ™ºèƒ½åŠ¨æ€æƒé‡ (ä¿®å¤ç‰ˆ)...")
    
    # è¯»å–æ•°æ®
    data = pd.read_csv(csv_file)
    total_samples = len(data)
    
    weights = []
    print(f"\nğŸ“Š å„æ ‡ç­¾æƒé‡è®¡ç®—:")
    print("-" * 60)
    print(f"{'æ ‡ç­¾':<25} {'æ­£æ ·æœ¬æ•°':<8} {'è´Ÿæ ·æœ¬æ•°':<8} {'æƒé‡':<8}")
    print("-" * 60)
    
    for label in labels:
        if label in data.columns:
            pos_count = (data[label] == 1).sum()
            neg_count = total_samples - pos_count
            
            if pos_count > 0:
                # ä½¿ç”¨æ›´æ¸©å’Œçš„æƒé‡è®¡ç®—ï¼šå¯¹æ•°å¹³æ»‘ + é™åˆ¶æœ€å¤§å€¼
                raw_weight = neg_count / pos_count
                # å¯¹æ•°å¹³æ»‘ï¼Œé¿å…æç«¯å€¼
                weight = min(np.log(raw_weight + 1) + 1.0, 10.0)  # é™åˆ¶æœ€å¤§æƒé‡ä¸º10
            else:
                weight = 1.0
                
            weights.append(weight)
            print(f"{label:<25} {pos_count:<8} {neg_count:<8} {weight:<8.2f}")
        else:
            weights.append(1.0)
            print(f"{label:<25} {'N/A':<8} {'N/A':<8} {1.0:<8.2f}")
    
    print("-" * 60)
    print(f"âœ… æƒé‡è®¡ç®—å®Œæˆï¼Œå¹³å‡æƒé‡: {np.mean(weights):.2f}")
    print(f"ğŸ’¡ ä½¿ç”¨ä¿å®ˆæƒé‡ç­–ç•¥ï¼Œæœ€å¤§æƒé‡é™åˆ¶ä¸º10.0")
    
    return torch.FloatTensor(weights)

class CheXpertDataset(Dataset):
    """
    CheXpertæ•°æ®é›†ç±»
    æ”¯æŒå¤šæ ‡ç­¾åˆ†ç±»è®­ç»ƒ
    """
    def __init__(self, csv_file, image_dir, transform=None):
        self.data = pd.read_csv(csv_file)
        self.image_dir = Path(image_dir)
        self.transform = transform
        
        # CheXpert 14ä¸ªæ ‡ç­¾
        self.labels = [
            'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 
            'Lung Lesion', 'Lung Opacity', 'Edema', 'Consolidation', 
            'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 
            'Pleural Other', 'Fracture', 'Support Devices'
        ]
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        
        # ä»å®é™…çš„æ•°æ®ç»“æ„åŠ è½½å›¾åƒ
        # å®é™…ç»“æ„: E:/data_subset1/p10000032/s50414267/
        subject_id = str(row['subject_id'])
        study_id = str(row['study_id'])
        
        # æ„å»ºç›®å½•è·¯å¾„
        patient_dir = f"p{subject_id}"       # p10000032
        study_dir = f"s{study_id}"           # s50414267
        
        study_path = self.image_dir / patient_dir / study_dir
        
        try:
            # æŸ¥æ‰¾studyç›®å½•ä¸­çš„DICOMæ–‡ä»¶
            if study_path.exists():
                # å¯»æ‰¾å¯èƒ½çš„å›¾åƒæ–‡ä»¶
                image_files = []
                for ext in ['.dcm', '.jpg', '.png', '.jpeg']:
                    image_files.extend(list(study_path.glob(f"*{ext}")))
                
                if image_files:
                    image_path = image_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„å›¾åƒæ–‡ä»¶
                    
                    if image_path.suffix.lower() == '.dcm':
                        # å¤„ç†DICOMæ–‡ä»¶
                        import pydicom
                        dicom_data = pydicom.dcmread(str(image_path))
                        image_array = dicom_data.pixel_array
                        
                        # è½¬æ¢ä¸ºPILå›¾åƒ
                        if len(image_array.shape) == 2:  # ç°åº¦å›¾åƒ
                            from PIL import Image
                            # å½’ä¸€åŒ–åˆ°0-255
                            if image_array.max() > image_array.min():
                                image_array = ((image_array - image_array.min()) / 
                                              (image_array.max() - image_array.min()) * 255).astype(np.uint8)
                            else:
                                image_array = np.zeros_like(image_array, dtype=np.uint8)
                            image = Image.fromarray(image_array, mode='L').convert('RGB')
                        else:
                            raise ValueError("Unexpected image array shape")
                    else:
                        # å¤„ç†æ™®é€šå›¾åƒæ–‡ä»¶
                        from PIL import Image
                        image = Image.open(image_path).convert('RGB')
                else:
                    # å¦‚æœæ²¡æ‰¾åˆ°å›¾åƒæ–‡ä»¶ï¼Œåˆ›å»ºå ä½å›¾åƒ
                    print(f"Warning: No image files found in {study_path}")
                    from PIL import Image
                    image = Image.new('RGB', (224, 224), color=(128, 128, 128))
            else:
                # å¦‚æœstudyç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºå ä½å›¾åƒ
                print(f"Warning: Study directory not found: {study_path}")
                from PIL import Image
                image = Image.new('RGB', (224, 224), color=(64, 64, 64))
                
        except Exception as e:
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œåˆ›å»ºå ä½å›¾åƒ
            print(f"Warning: Could not load image from {study_path}: {e}")
            from PIL import Image
            image = Image.new('RGB', (224, 224), color=(192, 192, 192))
        
        # åº”ç”¨transform
        if self.transform:
            image = self.transform(image)
        
        # æå–æ ‡ç­¾ (å°†-1ä¸ç¡®å®šæ ‡ç­¾è½¬ä¸º0)
        labels = []
        for label in self.labels:
            value = row.get(label, 0)
            # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„æ ‡ç­¾å€¼
            if isinstance(value, str):
                if value == '1.0' or value == '1':
                    labels.append(1)
                else:
                    labels.append(0)
            else:
                labels.append(1 if value == 1 else 0)  # å°†-1å’Œ0éƒ½è§†ä¸º0
        
        return image, torch.FloatTensor(labels)

def create_data_splits(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42):
    """
    åˆ›å»ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†åˆ’åˆ†
    
    Args:
        dataset: å®Œæ•´æ•°æ®é›†
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹ (é»˜è®¤70%)
        val_ratio: éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤15%)
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹ (é»˜è®¤15%)
        random_state: éšæœºç§å­
    
    Returns:
        train_dataset, val_dataset, test_dataset
    """
    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, "æ¯”ä¾‹æ€»å’Œå¿…é¡»ä¸º1"
    
    # è·å–æ‰€æœ‰ç´¢å¼•
    indices = list(range(len(dataset)))
    
    # åˆ†å±‚é‡‡æ · - åŸºäº'No Finding'æ ‡ç­¾æ¥ä¿è¯ç±»åˆ«å¹³è¡¡
    labels = []
    for i in indices:
        _, label_tensor = dataset[i]
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ‡ç­¾(No Finding)ä½œä¸ºåˆ†å±‚åŸºå‡†
        labels.append(int(label_tensor[0].item()))
    
    # ç¬¬ä¸€æ¬¡åˆ’åˆ†ï¼šåˆ†ç¦»è®­ç»ƒé›†å’Œå‰©ä½™éƒ¨åˆ†
    train_indices, temp_indices = train_test_split(
        indices, 
        test_size=(val_ratio + test_ratio),
        random_state=random_state,
        stratify=labels
    )
    
    # è®¡ç®—éªŒè¯é›†å’Œæµ‹è¯•é›†åœ¨å‰©ä½™æ•°æ®ä¸­çš„æ¯”ä¾‹
    temp_val_ratio = val_ratio / (val_ratio + test_ratio)
    
    # è·å–å‰©ä½™éƒ¨åˆ†çš„æ ‡ç­¾
    temp_labels = [labels[i] for i in temp_indices]
    
    # ç¬¬äºŒæ¬¡åˆ’åˆ†ï¼šåˆ†ç¦»éªŒè¯é›†å’Œæµ‹è¯•é›†
    val_indices, test_indices = train_test_split(
        temp_indices,
        test_size=(1 - temp_val_ratio),
        random_state=random_state,
        stratify=temp_labels
    )
    
    # åˆ›å»ºå­é›†
    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)
    test_dataset = Subset(dataset, test_indices)
    
    print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
    print(f"  è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬ ({len(train_dataset)/len(dataset)*100:.1f}%)")
    print(f"  éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬ ({len(val_dataset)/len(dataset)*100:.1f}%)")
    print(f"  æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬ ({len(test_dataset)/len(dataset)*100:.1f}%)")
    
    return train_dataset, val_dataset, test_dataset

class DenseNetCheXpert(nn.Module):
    """
    åŸºäºDenseNet-121çš„CheXpertåˆ†ç±»å™¨ - é˜¶æ®µ2ä¼˜åŒ–ç‰ˆæœ¬
    Supporting Slide 5: Model Core - Building a Precise Diagnostic Engine
    """
    def __init__(self, num_classes=14, pretrained=True):
        super(DenseNetCheXpert, self).__init__()
        
        # ä¸»å¹²ç½‘ç»œï¼šDenseNet-121 (é«˜å‚æ•°æ•ˆç‡ï¼Œç‰¹å¾ä¼ æ’­å¥½)
        self.backbone = models.densenet121(pretrained=pretrained)
        
        # è·å–ç‰¹å¾ç»´åº¦
        num_features = self.backbone.classifier.in_features  # 1024
        
        # æ”¹è¿›çš„å¤šå±‚åˆ†ç±»å¤´ - æ¸è¿›å¼é™ç»´ + æ®‹å·®è¿æ¥
        self.classifier = nn.Sequential(
            # ç¬¬ä¸€å±‚ï¼š1024 -> 512
            nn.Linear(num_features, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            
            # ç¬¬äºŒå±‚ï¼š512 -> 256
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            
            # è¾“å‡ºå±‚ï¼š256 -> 14
            nn.Linear(256, num_classes)
        )
        
        # ä¿å­˜ç‰¹å¾å›¾ç”¨äºGrad-CAM
        self.features = self.backbone.features
        
        # æ³¨å†Œhookä»¥è·å–ç‰¹å¾å›¾
        self.feature_maps = None
        self.gradients = None
        
    def forward(self, x):
        # å‰å‘ä¼ æ’­å¹¶ä¿å­˜ç‰¹å¾å›¾
        features = self.features(x)
        
        # æ³¨å†Œhookè·å–æ¢¯åº¦
        if features.requires_grad:
            self.feature_maps = features
            features.register_hook(self.save_gradients)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        pooled = nn.functional.adaptive_avg_pool2d(features, (1, 1))
        pooled = torch.flatten(pooled, 1)
        
        # ç®€åŒ–çš„å•å±‚åˆ†ç±»å™¨
        output = self.classifier(pooled)
        
        return output
    
    def save_gradients(self, grad):
        """ä¿å­˜æ¢¯åº¦ç”¨äºGrad-CAM"""
        self.gradients = grad

class CheXpertTrainer:
    """
    CheXpertæ¨¡å‹è®­ç»ƒå™¨ - é˜¶æ®µ2ä¼˜åŒ–ç‰ˆæœ¬
    Supporting Slide 5: Training Details
    """
    def __init__(self, model, device='cuda', csv_file=None):
        self.model = model.to(device)
        self.device = device
        
        # CheXpert 14ä¸ªæ ‡ç­¾
        self.labels = [
            'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 
            'Lung Lesion', 'Lung Opacity', 'Edema', 'Consolidation', 
            'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 
            'Pleural Other', 'Fracture', 'Support Devices'
        ]
        
        # è®¡ç®—æ™ºèƒ½åŠ¨æ€æƒé‡
        if csv_file:
            pos_weights = calculate_class_weights(csv_file, self.labels)
        else:
            # å¤‡ç”¨æƒé‡ï¼ˆå¦‚æœæ²¡æœ‰æä¾›CSVæ–‡ä»¶ï¼‰
            pos_weights = torch.FloatTensor([30.0, 50.0, 6.0, 35.0, 3.0, 15.0, 
                                           10.0, 20.0, 8.0, 25.0, 4.0, 40.0, 45.0, 2.0])
        
        print(f"ğŸ’¡ ä½¿ç”¨æ™ºèƒ½åŠ¨æ€æƒé‡: {pos_weights.numpy()}")
        
        # ä½¿ç”¨æ¸©å’Œçš„ç„¦ç‚¹æŸå¤±æ›¿ä»£BCEWithLogitsLoss
        self.criterion = FocalLoss(
            alpha=0.25,  # æ›´æ¸©å’Œçš„æ­£æ ·æœ¬æƒé‡
            gamma=1.0,   # é™ä½å›°éš¾æ ·æœ¬ä¸“æ³¨åº¦
            pos_weight=pos_weights.to(device)
        )
        
        # ä¼˜åŒ–å™¨ï¼šAdam with conservative learning rate
        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-4, weight_decay=1e-4)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šç¨³å®šçš„StepLR
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer, 
            step_size=10,   # æ¯10ä¸ªepoché™ä½å­¦ä¹ ç‡
            gamma=0.3       # å­¦ä¹ ç‡ä¹˜ä»¥0.3
        )
        
        # ç§»é™¤Warmupå‚æ•°
        self.warmup_epochs = 0
        self.base_lr = 1e-4
        self.warmup_lr = 1e-5
        
        self.train_losses = []
        self.val_losses = []
        self.current_epoch = 0
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch - åŒ…å«warmupæœºåˆ¶"""
        self.model.train()
        total_loss = 0
        
        # Warmupå­¦ä¹ ç‡è°ƒæ•´
        if self.current_epoch < self.warmup_epochs:
            lr_scale = (self.current_epoch + 1) / self.warmup_epochs
            warmup_lr = self.warmup_lr + (self.base_lr - self.warmup_lr) * lr_scale
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = warmup_lr
            print(f"ğŸ”¥ Warmupé˜¶æ®µ: å­¦ä¹ ç‡ = {warmup_lr:.6f}")
        
        try:
            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = data.to(self.device), target.to(self.device)
                
                self.optimizer.zero_grad()
                output = self.model(data)
                loss = self.criterion(output, target)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                
                total_loss += loss.item()
                
                if batch_idx % 10 == 0:
                    current_lr = self.optimizer.param_groups[0]['lr']
                    print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}, LR: {current_lr:.6f}')
                
                # æ¸…ç†GPUå†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            print("ğŸ’¡ å°è¯•å‡å°‘æ‰¹æ¬¡å¤§å°æˆ–æ£€æŸ¥GPUå†…å­˜")
            raise e
        
        avg_loss = total_loss / len(train_loader)
        self.train_losses.append(avg_loss)
        return avg_loss
    
    def validate(self, val_loader):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        correct_predictions = 0
        total_predictions = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                loss = self.criterion(output, target)
                total_loss += loss.item()
                
                # è®¡ç®—å‡†ç¡®ç‡
                predictions = torch.sigmoid(output) > 0.5
                correct_predictions += (predictions == target.bool()).sum().item()
                total_predictions += target.numel()
        
        avg_loss = total_loss / len(val_loader)
        accuracy = correct_predictions / total_predictions
        
        self.val_losses.append(avg_loss)
        
        # åªåœ¨warmupé˜¶æ®µä¹‹åè°ƒç”¨scheduler
        if self.current_epoch >= self.warmup_epochs:
            self.scheduler.step()
        
        return avg_loss, accuracy
    
    def find_optimal_thresholds(self, val_loader):
        """åŸºäºéªŒè¯é›†å¯»æ‰¾æ¯ä¸ªæ ‡ç­¾çš„æœ€ä¼˜é˜ˆå€¼"""
        print("ğŸ” æ­£åœ¨å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼...")
        
        self.model.eval()
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                predictions = torch.sigmoid(output)
                all_predictions.append(predictions.cpu().numpy())
                all_targets.append(target.cpu().numpy())
        
        all_predictions = np.vstack(all_predictions)
        all_targets = np.vstack(all_targets)
        
        # ä¸ºæ¯ä¸ªæ ‡ç­¾å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
        optimal_thresholds = []
        from sklearn.metrics import precision_recall_curve, f1_score
        
        print("\nğŸ“Š å„æ ‡ç­¾æœ€ä¼˜é˜ˆå€¼:")
        print("-" * 60)
        print(f"{'æ ‡ç­¾':<25} {'æœ€ä¼˜é˜ˆå€¼':<10} {'æœ€ä¼˜F1':<10}")
        print("-" * 60)
        
        for i, label in enumerate(self.labels):
            try:
                if np.sum(all_targets[:, i]) > 0:  # ç¡®ä¿æœ‰æ­£æ ·æœ¬
                    # ä½¿ç”¨F1åˆ†æ•°å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
                    precision, recall, thresholds = precision_recall_curve(all_targets[:, i], all_predictions[:, i])
                    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
                    
                    best_threshold_idx = np.argmax(f1_scores)
                    best_threshold = thresholds[best_threshold_idx] if best_threshold_idx < len(thresholds) else 0.5
                    best_f1 = f1_scores[best_threshold_idx]
                    
                    optimal_thresholds.append(best_threshold)
                    print(f"{label:<25} {best_threshold:<10.3f} {best_f1:<10.3f}")
                else:
                    optimal_thresholds.append(0.5)  # é»˜è®¤é˜ˆå€¼
                    print(f"{label:<25} {0.5:<10.3f} {'N/A':<10}")
            except:
                optimal_thresholds.append(0.5)
                print(f"{label:<25} {0.5:<10.3f} {'Error':<10}")
        
        print("-" * 60)
        return np.array(optimal_thresholds)

    def evaluate_test_set(self, test_loader, optimal_thresholds=None):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ - æ”¯æŒæœ€ä¼˜é˜ˆå€¼"""
        print("ğŸ§ª åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
        
        self.model.eval()
        all_predictions = []
        all_targets = []
        total_loss = 0
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                loss = self.criterion(output, target)
                total_loss += loss.item()
                
                # æ”¶é›†é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
                predictions = torch.sigmoid(output)
                all_predictions.append(predictions.cpu().numpy())
                all_targets.append(target.cpu().numpy())
        
        # åˆå¹¶æ‰€æœ‰é¢„æµ‹å’Œæ ‡ç­¾
        all_predictions = np.vstack(all_predictions)
        all_targets = np.vstack(all_targets)
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        avg_loss = total_loss / len(test_loader)
        
        # è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„AUCã€ç²¾ç¡®åº¦ã€å¬å›ç‡ç­‰
        from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score
        
        labels = [
            'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 
            'Lung Lesion', 'Lung Opacity', 'Edema', 'Consolidation', 
            'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 
            'Pleural Other', 'Fracture', 'Support Devices'
        ]
        
        print(f"\nğŸ“Š æµ‹è¯•é›†è¯„ä¼°ç»“æœ:")
        print(f"å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        # å¦‚æœæä¾›äº†æœ€ä¼˜é˜ˆå€¼ï¼Œä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è¯„ä¼°
        if optimal_thresholds is not None:
            print("\nğŸ¯ ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼çš„è¯„ä¼°ç»“æœ:")
            print("-" * 90)
            print(f"{'æ ‡ç­¾':<25} {'AUC':<6} {'ç²¾ç¡®åº¦':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8} {'é˜ˆå€¼':<8}")
            print("-" * 90)
        else:
            print("\nä½¿ç”¨å›ºå®š0.5é˜ˆå€¼çš„è¯„ä¼°ç»“æœ:")
            print("-" * 80)
            print(f"{'æ ‡ç­¾':<25} {'AUC':<6} {'ç²¾ç¡®åº¦':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8}")
            print("-" * 80)
        
        auc_scores = []
        for i, label in enumerate(labels):
            try:
                # AUC
                auc = roc_auc_score(all_targets[:, i], all_predictions[:, i])
                auc_scores.append(auc)
                
                # äºŒå€¼åŒ–é¢„æµ‹ç»“æœ
                if optimal_thresholds is not None:
                    threshold = optimal_thresholds[i]
                    pred_binary = (all_predictions[:, i] > threshold).astype(int)
                else:
                    threshold = 0.5
                    pred_binary = (all_predictions[:, i] > threshold).astype(int)
                
                # ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°
                precision = precision_score(all_targets[:, i], pred_binary, zero_division=0)
                recall = recall_score(all_targets[:, i], pred_binary, zero_division=0)
                f1 = f1_score(all_targets[:, i], pred_binary, zero_division=0)
                
                if optimal_thresholds is not None:
                    print(f"{label:<25} {auc:<6.3f} {precision:<8.3f} {recall:<8.3f} {f1:<8.3f} {threshold:<8.3f}")
                else:
                    print(f"{label:<25} {auc:<6.3f} {precision:<8.3f} {recall:<8.3f} {f1:<8.3f}")
                
            except Exception as e:
                print(f"{label:<25} Error: {str(e)[:40]}")
                auc_scores.append(0)
        
        mean_auc = np.mean(auc_scores)
        if optimal_thresholds is not None:
            print("-" * 90)
        else:
            print("-" * 80)
        print(f"å¹³å‡AUC: {mean_auc:.3f}")
        
        return avg_loss, mean_auc

    def train(self, train_loader, val_loader, test_loader=None, epochs=12):
        """å®Œæ•´è®­ç»ƒæµç¨‹ - é˜¶æ®µ2ä¼˜åŒ–ç‰ˆæœ¬"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒ CheXpert DenseNet-121 æ¨¡å‹ (é˜¶æ®µ2ä¼˜åŒ–)...")
        print("ğŸ’¡ ä½¿ç”¨ç„¦ç‚¹æŸå¤± + åŠ¨æ€æƒé‡ + CosineAnnealingWarmRestarts + Warmup...")
        
        best_val_loss = float('inf')
        best_auc = 0.0
        patience_counter = 0
        early_stop_patience = 6  # å¢åŠ è€å¿ƒå€¼ç»™æ–°ç­–ç•¥æ›´å¤šæ—¶é—´
        
        for epoch in range(epochs):
            self.current_epoch = epoch
            print(f"\nEpoch {epoch+1}/{epochs}")
            print("-" * 50)
            
            # è®­ç»ƒ
            train_loss = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss, val_acc = self.validate(val_loader)
            
            current_lr = self.optimizer.param_groups[0]['lr']
            print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
            print(f"å½“å‰å­¦ä¹ ç‡: {current_lr:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ (åŸºäºéªŒè¯æŸå¤±)
            is_best = val_loss < best_val_loss
            if is_best:
                best_val_loss = val_loss
                patience_counter = 0
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'val_loss': val_loss,
                    'val_accuracy': val_acc,
                    'scheduler_state_dict': self.scheduler.state_dict()
                }, 'best_chexpert_densenet121_v2.pth')
                print("âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (v2)")
            else:
                patience_counter += 1
                if patience_counter >= early_stop_patience:
                    print(f"â¹ï¸ æ—©åœè§¦å‘ (patience={early_stop_patience})")
                    break
        
        print("ğŸ‰ é˜¶æ®µ2ä¼˜åŒ–è®­ç»ƒå®Œæˆ!")
        
        # å¦‚æœæä¾›äº†æµ‹è¯•é›†ï¼Œè¿›è¡Œæœ€ç»ˆè¯„ä¼°
        if test_loader is not None:
            print("\n" + "="*60)
            # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•
            try:
                checkpoint = torch.load('best_chexpert_densenet121_v2.pth')
                self.model.load_state_dict(checkpoint['model_state_dict'])
                print("âœ… åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡è¿›è¡Œæµ‹è¯•é›†è¯„ä¼° (v2)")
            except:
                print("âš ï¸ ä½¿ç”¨å½“å‰æ¨¡å‹æƒé‡è¿›è¡Œæµ‹è¯•é›†è¯„ä¼°")
            
            # ğŸ” é¦–å…ˆåœ¨éªŒè¯é›†ä¸Šå¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
            print("\nğŸ¯ Step 1: åœ¨éªŒè¯é›†ä¸Šå¯»æ‰¾æœ€ä¼˜é˜ˆå€¼...")
            optimal_thresholds = self.find_optimal_thresholds(val_loader)
            
            # ğŸ§ª ç„¶ååœ¨æµ‹è¯•é›†ä¸Šä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è¯„ä¼°
            print("\nğŸ¯ Step 2: ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
            test_loss, test_auc = self.evaluate_test_set(test_loader, optimal_thresholds)
            
            # ğŸ“Š å¯¹æ¯”å›ºå®šé˜ˆå€¼å’Œæœ€ä¼˜é˜ˆå€¼çš„æ•ˆæœ
            print("\nğŸ¯ Step 3: å¯¹æ¯”å›ºå®šé˜ˆå€¼æ•ˆæœ...")
            test_loss_fixed, test_auc_fixed = self.evaluate_test_set(test_loader, None)
            
            print(f"\nğŸ“ˆ é˜ˆå€¼ä¼˜åŒ–æ•ˆæœå¯¹æ¯”:")
            print(f"  å›ºå®šé˜ˆå€¼(0.5) AUC: {test_auc_fixed:.3f}")
            print(f"  æœ€ä¼˜é˜ˆå€¼ AUC: {test_auc:.3f}")
            print(f"  AUCæå‡: {test_auc - test_auc_fixed:+.3f}")
            
            return test_loss, test_auc
        else:
            return None, None

class GradCAMExplainer:
    """
    Grad-CAMå¯è§£é‡Šæ€§åˆ†æå™¨
    Supporting Slide 6: Explainable AI (XAI)
    """
    def __init__(self, model, device='cuda'):
        self.model = model.to(device)
        self.device = device
        self.model.eval()
        
        # CheXpertæ ‡ç­¾
        self.labels = [
            'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 
            'Lung Lesion', 'Lung Opacity', 'Edema', 'Consolidation', 
            'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 
            'Pleural Other', 'Fracture', 'Support Devices'
        ]
    
    def generate_gradcam(self, image, class_idx):
        """
        ç”ŸæˆGrad-CAMçƒ­åŠ›å›¾
        æ‰“å¼€"é»‘ç®±"ï¼šè®©AIçš„è¯Šæ–­çœ‹å¾—æ‡‚
        """
        # å‰å‘ä¼ æ’­
        image = image.unsqueeze(0).to(self.device)
        image.requires_grad_()
        
        output = self.model(image)
        
        # åå‘ä¼ æ’­
        self.model.zero_grad()
        class_score = output[0, class_idx]
        class_score.backward()
        
        # è·å–ç‰¹å¾å›¾å’Œæ¢¯åº¦
        feature_maps = self.model.feature_maps   # [1, 1024, 7, 7]
        gradients = self.model.gradients        # [1, 1024, 7, 7]
        
        # è®¡ç®—æƒé‡ (å…¨å±€å¹³å‡æ± åŒ–æ¢¯åº¦)
        weights = torch.mean(gradients, dim=(2, 3))  # [1, 1024]
        
        # ç”Ÿæˆçƒ­åŠ›å›¾
        cam = torch.zeros(
            feature_maps.shape[2:],
            dtype=feature_maps.dtype,
            device=feature_maps.device
        )
        for i in range(weights.shape[1]):
            cam += weights[0, i] * feature_maps[0, i]
        
        # ReLUæ¿€æ´»
        cam = torch.relu(cam)
        
        # å½’ä¸€åŒ–åˆ°[0,1]
        if cam.max() > 0:
            cam = cam / cam.max()
        
        return cam.cpu().numpy()
    
    def visualize_gradcam(self, image_path, class_idx, save_path=None):
        """
        å¯è§†åŒ–Grad-CAMç»“æœ
        å°†æŠ½è±¡çš„é¢„æµ‹è½¬åŒ–ä¸ºç›´è§‚çš„è§†è§‰è¯æ®
        """
        # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        original_image = Image.open(image_path).convert('RGB')
        input_image = transform(original_image)
        
        # ç”Ÿæˆçƒ­åŠ›å›¾
        cam = self.generate_gradcam(input_image, class_idx)
        
        # è°ƒæ•´çƒ­åŠ›å›¾å¤§å°
        cam_resized = cv2.resize(cam, (224, 224))
        
        # åˆ›å»ºå¯è§†åŒ–
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        # åŸå§‹å›¾åƒ
        axes[0].imshow(original_image)
        axes[0].set_title('åŸå§‹èƒ¸éƒ¨Xå…‰å›¾åƒ')
        axes[0].axis('off')
        
        # çƒ­åŠ›å›¾
        im1 = axes[1].imshow(cam_resized, cmap='jet')
        axes[1].set_title(f'Grad-CAM: {self.labels[class_idx]}')
        axes[1].axis('off')
        plt.colorbar(im1, ax=axes[1])
        
        # å åŠ å›¾åƒ
        overlay = np.array(original_image.resize((224, 224)))
        cam_colored = plt.cm.jet(cam_resized)[:, :, :3]
        overlay_result = 0.6 * overlay/255.0 + 0.4 * cam_colored
        
        axes[2].imshow(overlay_result)
        axes[2].set_title('çƒ­åŠ›å›¾å åŠ  (å…³æ³¨åŒºåŸŸé«˜äº®)')
        axes[2].axis('off')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… Grad-CAMå¯è§†åŒ–ä¿å­˜åˆ°: {save_path}")
        
        plt.show()
        
        return cam_resized
    
    def explain_prediction(self, image_path, top_k=3):
        """
        å…¨é¢è§£é‡Šæ¨¡å‹é¢„æµ‹
        ç”Ÿæˆå¤šä¸ªæ ‡ç­¾çš„å¯è§£é‡Šæ€§åˆ†æ
        """
        # é¢„å¤„ç†å›¾åƒ
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        image = transform(Image.open(image_path).convert('RGB'))
        image_tensor = image.unsqueeze(0).to(self.device)
        
        # é¢„æµ‹
        with torch.no_grad():
            output = self.model(image_tensor)
            probabilities = torch.sigmoid(output).cpu().numpy()[0]
        
        # è·å–top-ké¢„æµ‹
        top_indices = np.argsort(probabilities)[-top_k:][::-1]
        
        print("ğŸ” AIè¯Šæ–­å¯è§£é‡Šæ€§åˆ†æ")
        print("=" * 50)
        
        for i, idx in enumerate(top_indices):
            prob = probabilities[idx]
            label = self.labels[idx]
            
            print(f"\n{i+1}. {label}")
            print(f"   é¢„æµ‹æ¦‚ç‡: {prob:.3f}")
            print(f"   ç½®ä¿¡åº¦: {'é«˜' if prob > 0.7 else 'ä¸­' if prob > 0.3 else 'ä½'}")
            
            # ç”Ÿæˆå¹¶ä¿å­˜Grad-CAM
            save_path = f"gradcam_{label.replace(' ', '_').lower()}.png"
            self.visualize_gradcam(image_path, idx, save_path)
        
        return top_indices, probabilities[top_indices]

# ä½¿ç”¨ç¤ºä¾‹å’Œè®­ç»ƒè„šæœ¬
def create_training_example():
    """åˆ›å»ºè®­ç»ƒç¤ºä¾‹ - ä½¿ç”¨å®é™…çš„æ•°æ®é›†è·¯å¾„"""
    print("ğŸ“š CheXpert DenseNet-121 è®­ç»ƒç¤ºä¾‹")
    print("Supporting Slide 5: Deep Learning Model Training")
    print("âœ… åŒ…å«è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†åˆ’åˆ†")
    print("âœ… ä½¿ç”¨å®é™…çš„æ•°æ®é›†æ–‡ä»¶")
    
    # å®é™…æ•°æ®è·¯å¾„
    csv_file = "e:/Learning/BN5212/data/labeled_reports_with_ids.csv"
    image_dir = "E:/data_subset1/"  # åŸå§‹å›¾åƒå­˜å‚¨ç›®å½•
    
    print(f"\nğŸ“‚ æ•°æ®è·¯å¾„:")
    print(f"  CSVæ–‡ä»¶: {csv_file}")
    print(f"  å›¾åƒç›®å½•: {image_dir}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    import os
    if not os.path.exists(csv_file):
        print(f"âŒ é”™è¯¯: CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
        print("ğŸ’¡ æç¤º: è¯·å…ˆè¿è¡Œæ•°æ®å¤„ç†è„šæœ¬ç”Ÿæˆæ ‡æ³¨æ•°æ®")
        return
    
    if not os.path.exists(image_dir):
        print(f"âŒ é”™è¯¯: å›¾åƒç›®å½•ä¸å­˜åœ¨: {image_dir}")
        print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥åŸå§‹æ•°æ®è·¯å¾„æ˜¯å¦æ­£ç¡®")
        return
    
    # é˜¶æ®µ2å¼ºåŒ–æ•°æ®å¢å¼º - ä¸“ä¸ºåŒ»å­¦å›¾åƒä¼˜åŒ–
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomCrop((224, 224)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=20),  # å¢åŠ æ—‹è½¬è§’åº¦
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.1),  # å¢å¼ºå¯¹æ¯”åº¦è°ƒæ•´
        transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.9, 1.1)),  # å¢åŠ ä»¿å°„å˜æ¢
        transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.3),  # æ–°å¢ï¼šé”åº¦è°ƒæ•´
        transforms.RandomAutocontrast(p=0.2),  # æ–°å¢ï¼šè‡ªåŠ¨å¯¹æ¯”åº¦
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    val_test_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    # æ¼”ç¤ºæ•°æ®é›†åˆ’åˆ†é€»è¾‘
    print("\nğŸ“Š æ•°æ®é›†åˆ’åˆ†ç­–ç•¥:")
    print("  è®­ç»ƒé›†: 70% - ç”¨äºæ¨¡å‹è®­ç»ƒ")
    print("  éªŒè¯é›†: 15% - ç”¨äºè¶…å‚æ•°è°ƒä¼˜å’Œæ—©åœ")
    print("  æµ‹è¯•é›†: 15% - ç”¨äºæœ€ç»ˆæ€§èƒ½è¯„ä¼°")
    print("  âœ… ä½¿ç”¨åˆ†å±‚é‡‡æ ·ä¿è¯ç±»åˆ«å¹³è¡¡")
    
    try:
        # åˆ›å»ºå®Œæ•´æ•°æ®é›†
        print(f"\nğŸ”„ åŠ è½½æ•°æ®é›†...")
        full_dataset = CheXpertDataset(csv_file, image_dir, None)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(full_dataset)} ä¸ªæ ·æœ¬")
        
        # æ•°æ®é›†åˆ’åˆ†
        print(f"\nğŸ”„ æ‰§è¡Œæ•°æ®é›†åˆ’åˆ†...")
        train_dataset, val_dataset, test_dataset = create_data_splits(
            full_dataset, 
            train_ratio=0.7, 
            val_ratio=0.15, 
            test_ratio=0.15
        )
        
        # ä¸ºä¸åŒçš„æ•°æ®é›†åº”ç”¨ä¸åŒçš„transform
        print(f"\nğŸ”„ é…ç½®æ•°æ®å¢å¼º...")
        
        # é‡æ–°åˆ›å»ºå¸¦æœ‰transformçš„æ•°æ®é›†
        csv_file_path = csv_file
        image_dir_path = image_dir
        
        # åˆ›å»ºè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®é›†ï¼Œå„è‡ªä½¿ç”¨ä¸åŒçš„transform
        train_indices = train_dataset.indices
        val_indices = val_dataset.indices
        test_indices = test_dataset.indices
        
        # åˆ›å»ºå¸¦æœ‰ç‰¹å®štransformçš„è‡ªå®šä¹‰æ•°æ®é›†ç±»
        class TransformedSubset(Dataset):
            def __init__(self, original_dataset, indices, transform):
                self.original_dataset = original_dataset
                self.indices = indices
                self.transform = transform
            
            def __len__(self):
                return len(self.indices)
            
            def __getitem__(self, idx):
                original_idx = self.indices[idx]
                # è·å–åŸå§‹æ•°æ®ï¼ˆä¸åº”ç”¨transformï¼‰
                original_transform = self.original_dataset.transform
                self.original_dataset.transform = None
                image, labels = self.original_dataset[original_idx]
                self.original_dataset.transform = original_transform
                
                # åº”ç”¨æŒ‡å®šçš„transform
                if self.transform:
                    image = self.transform(image)
                
                return image, labels
        
        # åˆ›å»ºå¸¦æœ‰ä¸åŒtransformçš„æ•°æ®é›†
        train_dataset_transformed = TransformedSubset(full_dataset, train_indices, train_transform)
        val_dataset_transformed = TransformedSubset(full_dataset, val_indices, val_test_transform)
        test_dataset_transformed = TransformedSubset(full_dataset, test_indices, val_test_transform)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ (è°ƒæ•´æ‰¹æ¬¡å¤§å°é¿å…å†…å­˜é—®é¢˜)
        print(f"\nğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        train_loader = DataLoader(train_dataset_transformed, batch_size=8, shuffle=True, num_workers=0)
        val_loader = DataLoader(val_dataset_transformed, batch_size=8, shuffle=False, num_workers=0)
        test_loader = DataLoader(test_dataset_transformed, batch_size=8, shuffle=False, num_workers=0)
        
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        print(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
        print(f"  éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
        print(f"  æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½é”™è¯¯: {str(e)}")
        print("ğŸ’¡ å¯èƒ½åŸå› :")
        print("   1. å›¾åƒæ–‡ä»¶è·¯å¾„æ ¼å¼ä¸åŒ¹é…")
        print("   2. å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨æˆ–æŸå") 
        print("   3. CSVæ–‡ä»¶æ ¼å¼é—®é¢˜")
        return
    
    # åˆå§‹åŒ–æ¨¡å‹
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = DenseNetCheXpert(num_classes=14, pretrained=True)
    
    # åˆ›å»ºè®­ç»ƒå™¨ - ä¼ é€’CSVæ–‡ä»¶è·¯å¾„ç”¨äºè®¡ç®—åŠ¨æ€æƒé‡
    trainer = CheXpertTrainer(model, device, csv_file=csv_file)
    
    print(f"\nğŸ”§ ä¿®å¤è®­ç»ƒé…ç½®:")
    print(f"  âœ… æ¨¡å‹ç»“æ„: DenseNet-121 (ç®€åŒ–åˆ†ç±»å™¨)")
    print(f"  âœ… è®¾å¤‡: {device}")
    print(f"  âœ… æŸå¤±å‡½æ•°: FocalLoss (alpha=0.25, gamma=1.0) - æ¸©å’Œç‰ˆæœ¬")
    print(f"  âœ… ä¼˜åŒ–å™¨: Adam (lr=1e-4) - ä¿å®ˆå­¦ä¹ ç‡")
    print(f"  âœ… å­¦ä¹ ç‡è°ƒåº¦: StepLR (ç¨³å®šè°ƒåº¦)")
    print(f"  âœ… æ¢¯åº¦è£å‰ª: max_norm=1.0")
    print(f"  âœ… æ™ºèƒ½åŠ¨æ€æƒé‡: æœ€å¤§é™åˆ¶10.0 (ä¿å®ˆç‰ˆæœ¬)")
    print(f"  âœ… æ—©åœæœºåˆ¶: patience=6")
    print(f"  âœ… å¼ºåŒ–æ•°æ®å¢å¼º: åŒ»å­¦å›¾åƒä¸“ç”¨æŠ€æœ¯")
    
    print(f"\nğŸ“ˆ ä¿®å¤è®­ç»ƒæµç¨‹:")
    print("  1. ä¿å®ˆæƒé‡ï¼šé™åˆ¶æœ€å¤§æƒé‡ä¸º10.0ï¼Œé¿å…æç«¯å€¼")
    print("  2. ç¨³å®šå­¦ä¹ ç‡ï¼šä½¿ç”¨StepLRï¼Œæ¯10ä¸ªepochè¡°å‡30%")
    print("  3. æ¸©å’ŒæŸå¤±å‡½æ•°ï¼šé™ä½FocalLosså‚æ•°ï¼Œå‡å°‘è¿‡æ‹Ÿåˆ")
    print("  4. ç®€åŒ–æ¶æ„ï¼šç§»é™¤æ®‹å·®è¿æ¥ï¼Œå›å½’ç®€å•æœ‰æ•ˆè®¾è®¡")
    print("  5. æ¢¯åº¦è£å‰ªé˜²æ­¢è®­ç»ƒä¸ç¨³å®š")
    
    # å¼€å§‹ä¿®å¤è®­ç»ƒ
    print(f"\nğŸš€ å¼€å§‹ä¿®å¤è®­ç»ƒ...")
    print("ğŸ’¡ ç›®æ ‡ï¼šæ¢å¤åˆ°åŸºçº¿0.65+ AUCæ€§èƒ½...")
    test_loss, test_auc = trainer.train(
        train_loader, val_loader, test_loader, epochs=12)
    print(f'ğŸ¯ ä¿®å¤åæµ‹è¯•é›†æ€§èƒ½: æŸå¤±={test_loss:.4f}, å¹³å‡AUC={test_auc:.3f}')
    
    # æ€§èƒ½è¯„ä¼°
    if test_auc is not None:
        baseline_auc = 0.647
        failed_auc = 0.594
        improvement = test_auc - failed_auc
        vs_baseline = test_auc - baseline_auc
        print(f"\nğŸ“ˆ ä¿®å¤æ•ˆæœåˆ†æ:")
        print(f"  å¤±è´¥AUC: {failed_auc:.3f}")
        print(f"  åŸºçº¿AUC: {baseline_auc:.3f}")
        print(f"  ä¿®å¤åAUC: {test_auc:.3f}")
        print(f"  vså¤±è´¥ç‰ˆæœ¬: {improvement:+.3f}")
        print(f"  vsåŸºçº¿ç‰ˆæœ¬: {vs_baseline:+.3f}")
        if test_auc >= baseline_auc:
            print("  âœ… æˆåŠŸæ¢å¤åˆ°åŸºçº¿æ€§èƒ½")
        else:
            print("  âš ï¸ ä»æœªå®Œå…¨æ¢å¤ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæ•´")
    
    return train_loader, val_loader, test_loader, trainer

def create_data_split_demo():
    """æ¼”ç¤ºæ•°æ®é›†åˆ’åˆ†åŠŸèƒ½"""
    print("\nğŸ“Š æ•°æ®é›†åˆ’åˆ†æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†è¿›è¡Œæ¼”ç¤º
    class MockDataset:
        def __init__(self, size=1000):
            self.size = size
            # æ¨¡æ‹Ÿä¸å¹³è¡¡çš„æ ‡ç­¾åˆ†å¸ƒ
            np.random.seed(42)
            self.labels = np.random.choice([0, 1], size=size, p=[0.7, 0.3])
        
        def __len__(self):
            return self.size
        
        def __getitem__(self, idx):
            # è¿”å›æ¨¡æ‹Ÿçš„å›¾åƒtensorå’Œæ ‡ç­¾tensor
            mock_image = torch.randn(3, 224, 224)
            mock_labels = torch.zeros(14)
            mock_labels[0] = float(self.labels[idx])  # è½¬æ¢ä¸ºæµ®ç‚¹æ•°
            return mock_image, mock_labels
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†
    mock_dataset = MockDataset(1000)
    
    # æ¼”ç¤ºæ•°æ®é›†åˆ’åˆ†
    train_set, val_set, test_set = create_data_splits(
        mock_dataset,
        train_ratio=0.7,
        val_ratio=0.15, 
        test_ratio=0.15,
        random_state=42
    )
    
    print(f"âœ… æˆåŠŸåˆ’åˆ†æ¨¡æ‹Ÿæ•°æ®é›†")
    print(f"  æ€»æ ·æœ¬æ•°: {len(mock_dataset)}")
    print(f"  è®­ç»ƒé›†: {len(train_set)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(val_set)} æ ·æœ¬") 
    print(f"  æµ‹è¯•é›†: {len(test_set)} æ ·æœ¬")
    
    # éªŒè¯æ ‡ç­¾åˆ†å¸ƒ
    def check_label_distribution(dataset, name):
        labels = []
        for i in range(len(dataset)):
            _, label_tensor = dataset[i]
            labels.append(int(label_tensor[0].item()))
        
        pos_ratio = sum(labels) / len(labels)
        print(f"  {name} æ­£æ ·æœ¬æ¯”ä¾‹: {pos_ratio:.3f}")
        return pos_ratio
    
    print(f"\nğŸ” éªŒè¯ç±»åˆ«å¹³è¡¡:")
    orig_ratio = sum(mock_dataset.labels) / len(mock_dataset.labels)
    print(f"  åŸå§‹æ•°æ® æ­£æ ·æœ¬æ¯”ä¾‹: {orig_ratio:.3f}")
    
    train_ratio = check_label_distribution(train_set, "è®­ç»ƒé›†")
    val_ratio = check_label_distribution(val_set, "éªŒè¯é›†")
    test_ratio = check_label_distribution(test_set, "æµ‹è¯•é›†")
    
    print(f"âœ… åˆ†å±‚é‡‡æ ·æˆåŠŸä¿æŒç±»åˆ«å¹³è¡¡!")

def create_explainability_example():
    """åˆ›å»ºå¯è§£é‡Šæ€§åˆ†æç¤ºä¾‹"""
    print("ğŸ” Grad-CAMå¯è§£é‡Šæ€§åˆ†æç¤ºä¾‹")
    print("Supporting Slide 6: Explainable AI (XAI)")
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = DenseNetCheXpert(num_classes=14, pretrained=True)
    
    # åŠ è½½æƒé‡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    try:
        checkpoint = torch.load('best_chexpert_densenet121.pth', map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        print("âœ… åŠ è½½è®­ç»ƒå¥½çš„æƒé‡")
    except:
        print("âš ï¸ ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼ˆæœªé’ˆå¯¹CheXpertå¾®è°ƒï¼‰")
    
    # åˆ›å»ºå¯è§£é‡Šæ€§åˆ†æå™¨
    explainer = GradCAMExplainer(model, device)
    
    print("ğŸ¯ Grad-CAMåŠŸèƒ½:")
    print("  - ç”Ÿæˆçƒ­åŠ›å›¾ï¼Œæ˜¾ç¤ºæ¨¡å‹å…³æ³¨çš„å›¾åƒåŒºåŸŸ")
    print("  - å°†æŠ½è±¡é¢„æµ‹è½¬åŒ–ä¸ºç›´è§‚è§†è§‰è¯æ®")
    print("  - å¢å¼ºä¸´åºŠå¯ä¿¡åº¦å’Œè¯Šæ–­é€æ˜åº¦")
    
    # ç¤ºä¾‹ç”¨æ³•ï¼ˆéœ€è¦å®é™…çš„å›¾åƒæ–‡ä»¶ï¼‰
    # explainer.explain_prediction('sample_chest_xray.jpg', top_k=3)

if __name__ == "__main__":
    print("ğŸ¥ CheXpert AI è¯Šæ–­ç³»ç»Ÿ")
    print("=" * 50)
    
    # æ˜¾ç¤ºä¸»è¦åŠŸèƒ½
    print("ğŸ“‹ ä¸»è¦åŠŸèƒ½:")
    print("  1. DenseNet-121 æ·±åº¦å­¦ä¹ æ¨¡å‹")
    print("  2. 14ç§èƒ¸éƒ¨ç–¾ç—…å¤šæ ‡ç­¾åˆ†ç±»")
    print("  3. Grad-CAM å¯è§£é‡Šæ€§åˆ†æ")
    print("  4. âœ… è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†æ­£ç¡®åˆ’åˆ†")
    print("  5. å…¨é¢çš„æ¨¡å‹è¯„ä¼°æŒ‡æ ‡")
    
    # è¿è¡Œæ¼”ç¤º
    print("\n" + "="*50)
    create_data_split_demo()
    
    print("\n" + "="*50)
    create_training_example()
    
    print("\n" + "="*50)
    create_explainability_example()
    
    print(f"\nğŸ“Š æ€»ç»“:")
    print("âœ… å·²å®ç°properçš„æ•°æ®é›†åˆ’åˆ†ç­–ç•¥")
    print("âœ… æ”¯æŒåˆ†å±‚é‡‡æ ·ä¿è¯ç±»åˆ«å¹³è¡¡")
    print("âœ… åŒ…å«è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†å®Œæ•´æµç¨‹")
    print("âœ… æä¾›AUCã€ç²¾ç¡®åº¦ã€å¬å›ç‡ç­‰è¯„ä¼°æŒ‡æ ‡")
    print("âœ… é€‚åˆåŒ»å­¦å½±åƒAIçš„æœ€ä½³å®è·µ")